{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My name is Ahmed and Iam learning NLP.\n",
      "NLP has basic concept called Tokenization! That's it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "My name is Ahmed and Iam learning NLP.\n",
    "NLP has basic concept called Tokenization! That's it.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using NLTK (Natural Language Processing ToolKit) library\n",
    "- converting corpus(paragraph) into document(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\documents\\python\\nlp_genai\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\documents\\python\\nlp_genai\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\documents\\python\\nlp_genai\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\documents\\python\\nlp_genai\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\documents\\python\\nlp_genai\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tokenization\n",
    "- corpus -> paragraph\n",
    "- document -> sentence\n",
    "- words\n",
    "- vocabulary -> unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert corpus to document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus:\n",
      "My name is Ahmed and Iam learning NLP.\n",
      "NLP has basic concept called Tokenization! That's it.\n",
      "\n",
      "document:['\\nMy name is Ahmed and Iam learning NLP.', 'NLP has basic concept called Tokenization!', \"That's it.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "document = sent_tokenize(corpus)\n",
    "print(f\"corpus:{corpus}\")\n",
    "print(f\"document:{document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My name is Ahmed and Iam learning NLP.\n",
      "NLP has basic concept called Tokenization!\n",
      "That's it.\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert document to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: \n",
      "My name is Ahmed and Iam learning NLP.\n",
      "NLP has basic concept called Tokenization! That's it.\n",
      "\n",
      "words: ['My', 'name', 'is', 'Ahmed', 'and', 'Iam', 'learning', 'NLP', '.', 'NLP', 'has', 'basic', 'concept', 'called', 'Tokenization', '!', 'That', \"'s\", 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(corpus)\n",
    "\n",
    "print(f\"corpus: {corpus}\")\n",
    "print(f\"words: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "name\n",
      "is\n",
      "Ahmed\n",
      "and\n",
      "Iam\n",
      "learning\n",
      "NLP\n",
      ".\n",
      "NLP\n",
      "has\n",
      "basic\n",
      "concept\n",
      "called\n",
      "Tokenization\n",
      "!\n",
      "That\n",
      "'s\n",
      "it\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# in this case punctuation is not spliting like ('s) below\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Ahmed', 'and', 'Iam', 'learning', 'NLP', '.']\n",
      "['NLP', 'has', 'basic', 'concept', 'called', 'Tokenization', '!']\n",
      "['That', \"'s\", 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# convert sentences into words.\n",
    "for sentence in document:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can also seperate punctuation using wordpunct_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Ahmed', 'and', 'Iam', 'learning', 'NLP', '.', 'NLP', 'has', 'basic', 'concept', 'called', 'Tokenization', '!', 'That', \"'\", 's', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "split_punctuation = wordpunct_tokenize(corpus)\n",
    "print(split_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "name\n",
      "is\n",
      "Ahmed\n",
      "and\n",
      "Iam\n",
      "learning\n",
      "NLP\n",
      ".\n",
      "NLP\n",
      "has\n",
      "basic\n",
      "concept\n",
      "called\n",
      "Tokenization\n",
      "!\n",
      "That\n",
      "'\n",
      "s\n",
      "it\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# here punctuation is splited.\n",
    "for word in split_punctuation:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can't split \".\" using treebankWordTokenizer()\n",
    "- it only seperate last \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Ahmed',\n",
       " 'and',\n",
       " 'Iam',\n",
       " 'learning',\n",
       " 'NLP.',\n",
       " 'NLP',\n",
       " 'has',\n",
       " 'basic',\n",
       " 'concept',\n",
       " 'called',\n",
       " 'Tokenization',\n",
       " '!',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- change the word to it's stem word \n",
    "- like convert (eating, eates) to it's stem word ('eat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating', 'eaten', 'ate', 'eats', 'writing', 'writes', 'programming', 'programs', 'history', 'finaly', 'finilized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "ate --> ate\n",
      "eats --> eat\n",
      "writing --> write\n",
      "writes --> write\n",
      "programming --> program\n",
      "programs --> program\n",
      "history --> histori\n",
      "finaly --> finali\n",
      "finilized --> finil\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, '-->', stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "rege_stemmer = RegexpStemmer('ing| s$| e$| able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rege_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rege_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "ate --> ate\n",
      "eats --> eat\n",
      "writing --> write\n",
      "writes --> write\n",
      "programming --> program\n",
      "programs --> program\n",
      "history --> histori\n",
      "finaly --> finali\n",
      "finilized --> finil\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"-->\" ,snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "### Wordnet Lemmatizer\n",
    "- Lemmatizer technique is like stemming. \n",
    "- Output we get after lemmatization is called lemma, which is root word rather than root stem like in stemming\n",
    "- wordnet lemmatizer takes more time then stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\hp/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    lemmatize('word', pos='n')\n",
    "    n use for Noun\n",
    "    v use for verb\n",
    "    a - adjective\n",
    "    r - adverb\n",
    "\"\"\"\n",
    "print(lemmatizer.lemmatize('going', 'v'))\n",
    "print(lemmatizer.lemmatize('going', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating', 'eaten', 'ate', 'eats', 'writing', 'writes', 'programming', 'programs', 'history', 'finaly', 'finilized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eat\n",
      "ate --> eat\n",
      "eats --> eat\n",
      "writing --> write\n",
      "writes --> write\n",
      "programming --> program\n",
      "programs --> program\n",
      "history --> history\n",
      "finaly --> finaly\n",
      "finilized --> finilized\n"
     ]
    }
   ],
   "source": [
    "# using lemmatization every word converts into it's root word.\n",
    "for word in words:\n",
    "    print(word,'-->',lemmatizer.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('fairly', 'v'), lemmatizer.lemmatize('sportingly', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWords with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Detecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\" These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval. By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text. In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nDetecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\"',\n",
       " 'These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval.',\n",
       " 'By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text.',\n",
       " 'In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\"\n",
      "These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval.\n",
      "By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text.\n",
      "In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Python',\n",
       " ',',\n",
       " 'libraries',\n",
       " 'like',\n",
       " 'NLTK',\n",
       " 'and',\n",
       " 'SpaCy',\n",
       " 'provide',\n",
       " 'pre-defined',\n",
       " 'lists',\n",
       " 'of',\n",
       " 'stop',\n",
       " 'words',\n",
       " ',',\n",
       " 'making',\n",
       " 'it',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'clean',\n",
       " 'and',\n",
       " 'preprocess',\n",
       " 'textual',\n",
       " 'data',\n",
       " 'for',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "nltk.word_tokenize(sentences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - change sentences into words\n",
    "# 2 - apply stemming on words, if word is not present in stopwords.words('english')\n",
    "# 3 - then add the stemmed words back into the sentence\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words_without_stopwords = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] =  \" \".join(words_without_stopwords) # converting all words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(words_without_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"detect stop word crucial step natur languag process ( nlp ) involv identifi remov commonli use word carri signific mean , `` , '' `` , '' `` , '' `` . ''\", 'these word typic filter improv effici accuraci text process task like text classif , sentiment analysi , inform retriev .', 'by elimin stop word , focu place meaning word contribut overal context content text .', 'in python , librari like nltk spaci provid pre-defin list stop word , make easier clean preprocess textual data nlp applic .']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - The above porter stemmer is not good so lets try snowball stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Detecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\" These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval. By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text. In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nDetecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\"',\n",
       " 'These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval.',\n",
       " 'By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text.',\n",
       " 'In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from paragraph to sentences tokenization\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - convert sentence into word\n",
    "# 2 - apply stemming on words other then stopwords\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i]) # convert sentence into words, so the multiple words pass through snow_stemmeing, so we keep in list\n",
    "    words_without_stopwords = [snow_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # check if word not in stopwords, then apply stemming\n",
    "    sentences[i] = ' '.join(words_without_stopwords) # join the all words and append to it's sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"detect stop word crucial step natur languag process ( nlp ) involv identifi remov common use word carri signific mean , `` , '' `` , '' `` , '' `` . ''\",\n",
       " 'these word typic filter improv effici accuraci text process task like text classif , sentiment analysi , inform retriev .',\n",
       " 'by elimin stop word , focus place meaning word contribut overal context content text .',\n",
       " 'in python , librari like nltk spaci provid pre-defin list stop word , make easier clean preprocess textual data nlp applic .']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - lets try with lemmatization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Detecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\" These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval. By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text. In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatization = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words_without_stopwords = [lemmatization.lemmatize(word.lower(), 'v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words_without_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"detect stop word crucial step natural language process ( nlp ) involve identify remove commonly use word carry significant mean , `` , '' `` , '' `` , '' `` . ''\",\n",
       " 'these word typically filter improve efficiency accuracy text process task like text classification , sentiment analysis , information retrieval .',\n",
       " 'by eliminate stop word , focus place meaningful word contribute overall context content text .',\n",
       " 'in python , libraries like nltk spacy provide pre-defined list stop word , make easier clean preprocess textual data nlp applications .']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization doesn't change words into lowercase, so we have to change words into lowercase using .lower()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part-of-Speech (POS) tags\n",
    "- It tells you whether a word is a noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Detecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\" These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval. By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text. In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nDetecting stop words is a crucial step in natural language processing (NLP) that involves identifying and removing commonly used words that do not carry significant meaning, such as \"is,\" \"and,\" \"the,\" and \"in.\"',\n",
       " 'These words are typically filtered out to improve the efficiency and accuracy of text processing tasks like text classification, sentiment analysis, and information retrieval.',\n",
       " 'By eliminating stop words, the focus is placed on more meaningful words that contribute to the overall context and content of the text.',\n",
       " 'In Python, libraries like NLTK and SpaCy provide pre-defined lists of stop words, making it easier to clean and preprocess textual data for NLP applications.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Detecting', 'VBG'), ('stop', 'NN'), ('words', 'NNS'), ('crucial', 'JJ'), ('step', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('involves', 'VBZ'), ('identifying', 'VBG'), ('removing', 'VBG'), ('commonly', 'RB'), ('used', 'VBN'), ('words', 'NNS'), ('carry', 'VBP'), ('significant', 'JJ'), ('meaning', 'NN'), (',', ','), ('``', '``'), (',', ','), (\"''\", \"''\"), ('``', '``'), (',', ','), (\"''\", \"''\"), ('``', '``'), (',', ','), (\"''\", \"''\"), ('``', '``'), ('.', '.'), (\"''\", \"''\")]\n",
      "[('These', 'DT'), ('words', 'NNS'), ('typically', 'RB'), ('filtered', 'VBD'), ('improve', 'VB'), ('efficiency', 'NN'), ('accuracy', 'NN'), ('text', 'IN'), ('processing', 'VBG'), ('tasks', 'NNS'), ('like', 'IN'), ('text', 'JJ'), ('classification', 'NN'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.')]\n",
      "[('By', 'IN'), ('eliminating', 'VBG'), ('stop', 'JJ'), ('words', 'NNS'), (',', ','), ('focus', 'NN'), ('placed', 'VBD'), ('meaningful', 'JJ'), ('words', 'NNS'), ('contribute', 'VBP'), ('overall', 'JJ'), ('context', 'NN'), ('content', 'NN'), ('text', 'NN'), ('.', '.')]\n",
      "[('In', 'IN'), ('Python', 'NNP'), (',', ','), ('libraries', 'VBZ'), ('like', 'IN'), ('NLTK', 'NNP'), ('SpaCy', 'NNP'), ('provide', 'VB'), ('pre-defined', 'JJ'), ('lists', 'NNS'), ('stop', 'VBP'), ('words', 'NNS'), (',', ','), ('making', 'VBG'), ('easier', 'JJR'), ('clean', 'JJ'), ('preprocess', 'NN'), ('textual', 'JJ'), ('data', 'NNS'), ('NLP', 'NNP'), ('applications', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # print(f\"words: {words}\")\n",
    "\n",
    "    words_without_stopwords = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    # print(f'words_without_stopwords:{words_without_stopwords}')\n",
    "    # print(f\"type:{type(words_without_stopwords)}\")\n",
    "\n",
    "    pos_tag = nltk.pos_tag(words_without_stopwords)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\hp/nltk_data', 'c:\\\\Users\\\\hp\\\\Documents\\\\Python\\\\NLP_GenAi\\\\venv\\\\nltk_data', 'c:\\\\Users\\\\hp\\\\Documents\\\\Python\\\\NLP_GenAi\\\\venv\\\\share\\\\nltk_data', 'c:\\\\Users\\\\hp\\\\Documents\\\\Python\\\\NLP_GenAi\\\\venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n",
      "Tagger found successfully!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)  # See where NLTK is looking for data\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle')\n",
    "    print(\"Tagger found successfully!\")\n",
    "except LookupError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NN')]\n",
      "[('Mahal', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('beautiful', 'NN')]\n",
      "[('Monument', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# assignment:\n",
    "sentence = 'Taj Mahal is a beautiful Monument'\n",
    "sentence = sentence.split()\n",
    "for word in sentence:\n",
    "    # print(word)\n",
    "    words = nltk.pos_tag([word])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag('Taj Mahal is a beautiful Monument'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition\n",
    "- it's technique to indentify or classify entitiy in text into pre-defined categories.\n",
    "- like person, organiation, location, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Elon Musk founded SpaceX in 2002 and Tesla in 2003.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "(S\n",
      "  (PERSON Elon/NNP)\n",
      "  (PERSON Musk/NNP)\n",
      "  founded/VBD\n",
      "  (ORGANIZATION SpaceX/NNP)\n",
      "  in/IN\n",
      "  2002/CD\n",
      "  and/CC\n",
      "  (GPE Tesla/NNP)\n",
      "  in/IN\n",
      "  2003/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# from nltk import draw_tree\n",
    "words = nltk.word_tokenize(sentence)\n",
    "print(type(words))\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(type(pos_tags))\n",
    "named_entities = nltk.ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
